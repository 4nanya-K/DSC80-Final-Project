{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNht40dW_P8I",
    "outputId": "54a815cb-d15e-4c02-a122-0be84b1f3196"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCt1QY2E5BJW"
   },
   "source": [
    "# Your Title Here\n",
    "\n",
    "**Name(s)**: Ananya Krishnan, John Wesley Pabalate\n",
    "\n",
    "**Website Link**: (your website link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    },
    "id": "U_OBhvSp5BJX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "#from dsc80_utils import * # Feel free to uncomment and use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqCArYDF5BJY"
   },
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqTPwukc5BJY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pxNfkQe5BJY"
   },
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "6SYdl1zN5BJY",
    "outputId": "b113df0d-d05d-471f-ea47-5ac64c8d3884"
   },
   "outputs": [],
   "source": [
    "#JohnWesley's Directory\n",
    "recipes = pd.read_csv('/Users/johnwesleypabalate/Desktop/dsc80-2025-wi/projects/project04/data/RAW_recipes.csv')\n",
    "\n",
    "#Ananya's Directory\n",
    "# recipes = pd.read_csv('/Users/ananyakrishnan/Downloads/DSC80/projects/project04/data/RAW_recipes.csv')\n",
    "# recipes.head()\n",
    "\n",
    "#Colab Directory\n",
    "# recipes = pd.read_csv('/content/RAW_recipes.csv')\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2WEHwDAEZTR"
   },
   "outputs": [],
   "source": [
    "recipes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fzm4nVcb5BJY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#JohnWesley's Directory\n",
    "reviews = pd.read_csv('/Users/johnwesleypabalate/Desktop/dsc80-2025-wi/projects/project04/data/RAW_interactions.csv')\n",
    "\n",
    "#Ananya's Directory\n",
    "# reviews = pd.read_csv('../DSC80/projects/project04/data/RAW_interactions.csv')\n",
    "# reviews.head()\n",
    "\n",
    "#Colab Directory\n",
    "# reviews = pd.read_csv('/content/interactions.csv')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1L0utkEEdNS"
   },
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj3R7Gms5BJY"
   },
   "source": [
    "Let us now merge recipes and ratings into one comprehensive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q813voGP5BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings = recipes.merge(reviews, left_on = 'id', right_on = 'recipe_id', how=\"left\")\n",
    "recipe_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okyx3cEWED9w"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbOaRYxY5dvM"
   },
   "outputs": [],
   "source": [
    "#Missingness Analysis Purposes\n",
    "merged_df = recipe_ratings.copy()\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3AtXzes5BJZ"
   },
   "source": [
    "Let us replace all 0s in the ratings column with NaN values. The 0 represents no rating given, but it will influence any calculations we perform with the ratings. We also calculate the average ratings for each recipe and store it in `avg_recipe_rating`. We will then add it as a column to recipe_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyIrGLYd5BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.loc[recipe_ratings['rating'] == 0, 'rating'] = np.nan\n",
    "avg_recipe_rating = recipe_ratings.groupby('recipe_id')['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyTvTB_y5BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings = recipe_ratings.merge(avg_recipe_rating.reset_index().rename(columns={'rating': 'avg_rating'}), on = 'recipe_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmih-YBt5BJZ"
   },
   "source": [
    "There are many columns not relevant to our question, so we will retain only the columns related to recipe id, nutrition information and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "herKkG_75BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings = recipe_ratings[['id', 'rating', 'avg_rating', 'nutrition']]\n",
    "recipe_ratings = recipe_ratings.rename(columns = {'id': 'recipe_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmhddbvL5BJZ"
   },
   "source": [
    "Let us now look at the columns we have and clean them up one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_uUcZ075BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atH8uKyK5BJZ"
   },
   "source": [
    "We observe that `nutrition` actually contains strings formatted to look like lists, so let us convert it to real lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    },
    "id": "O7Hpp81j5BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings['nutrition'] = recipe_ratings['nutrition'].str.strip('[').str.strip(']').str.replace(\"'\", \"\").str.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ire9fL1e5BJZ"
   },
   "source": [
    "The `nutrition` column now contains lists of values. Let us separate each value into its respective category - `'calories'`, `'total_fat'`, `'sugar'`, `'sodium'`, `'protein'`, `'saturated_fat'` and `'carbohydrates'`. We can then drop the `nutrition` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDMwYimz5BJa"
   },
   "outputs": [],
   "source": [
    "categories = ['calories', 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbohydrates']\n",
    "recipe_ratings = recipe_ratings.assign(\n",
    "    **{category: pd.to_numeric(recipe_ratings['nutrition'].str[i], errors='coerce') for i, category in enumerate(categories)}\n",
    ")\n",
    "recipe_ratings = recipe_ratings.drop(columns = ['nutrition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJtJCY025BJa"
   },
   "source": [
    "Let us now look at our cleaned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9pRY0ba5BJa"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5n-5JB45BJa"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XPjkiX_5BJa"
   },
   "source": [
    "The nutritional values seem to have abnormally high max values despite a reasonable mean. Let us look at the rows with high protein or high carbohydrate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwVNd8_X5BJa"
   },
   "outputs": [],
   "source": [
    "recipe_ratings[(recipe_ratings['protein'] > 200) | (recipe_ratings['carbohydrates'] > 200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K9OoPJK5BJa"
   },
   "source": [
    "These rows have proportionally high calories, meaning this is unlikely to be an error and could just be because of large portion sizes. We can leave it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU1BNU7T5BJa"
   },
   "source": [
    "# Ratio based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpBKrPCw5BJa"
   },
   "source": [
    "We will define high carbohydrate and low protein recipes as those in the top 25th percentile of carb to protein ratios. The carb to protein ratio provides a single measure that captures how carbohydrate-heavy a recipe is relative to its protein content.\n",
    "\n",
    "By taking the top 25% of the carb-to-protein ratio, we focus on recipes where carbohydrates are dominant relative to protein, regardless of total calories or fat content. Some recipes have 0 protein which will complicate our calculation of the ratios, so we will replace the 0s with 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNawEdEy5BJa"
   },
   "outputs": [],
   "source": [
    "ratio_recipe_ratings = recipe_ratings.copy()\n",
    "ratio_recipe_ratings['ratio_carb_protein'] = ratio_recipe_ratings['carbohydrates'] / ratio_recipe_ratings['protein'].replace(0, 0.1)\n",
    "ratio_recipe_ratings['ratio_carb_protein'] = ratio_recipe_ratings['ratio_carb_protein'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "ratio_recipe_ratings['high_carb_protein_ratio']= (ratio_recipe_ratings['ratio_carb_protein'] > ratio_recipe_ratings['ratio_carb_protein'].quantile(0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dicnjy3K5BJa"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nluSRsw_5BJa"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCYmEWJ45BJa"
   },
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    },
    "id": "0xjA6ww05BJb"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGyJMdXR5BJb"
   },
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    },
    "id": "k9YcSkDl5BJb"
   },
   "source": [
    "Our goal is to see if carbohydrate and protein content affect ratings of recipes. We define high carb-to-protein ratios as those in the top 25th percentile.\n",
    "\n",
    "**Null Hypothesis (H₀):** Recipes with a high carb-to-protein ratio receive the same ratings as other recipes.  \n",
    "\n",
    "**Alternative Hypothesis (Hₐ):** Recipes with a high carb-to-protein ratio receive significantly different ratings.\n",
    "\n",
    "**Test statistic:** Mean difference in ratings between the high-carb, low-protein group and all other recipes.\n",
    "\n",
    "**Significance level:** 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yP4r9XM5BJb"
   },
   "outputs": [],
   "source": [
    "observed_diff = ratio_recipe_ratings.groupby(\"high_carb_protein_ratio\")[\"avg_rating\"].mean().diff().iloc[-1]\n",
    "\n",
    "def permute_ratings(df):\n",
    "    shuffled = df[\"avg_rating\"].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    df[\"shuffled_rating\"] = shuffled\n",
    "    return df.groupby(\"high_carb_protein_ratio\")[\"shuffled_rating\"].mean().diff().iloc[-1]\n",
    "\n",
    "perm_diffs = [permute_ratings(ratio_recipe_ratings) for _ in range(1000)]\n",
    "\n",
    "p_value = np.mean(np.array(perm_diffs) >= observed_diff)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBJJYOr15BJb"
   },
   "source": [
    "Since the p-value is greater than the significance level 0.05, we fail to reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIN3pMnA5BJb"
   },
   "source": [
    "# Quantile based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_eH58cA5BJb"
   },
   "source": [
    "We want to account for the percentge of calories that the protein and carbohydrate contribute to. We will define high carbohydrate - low protein recipes using arbitrary cutoffs, considering those in the top 25th percentile of carb percentage and bottom 25th percentile of protein percentage. Using percentiles ensures that we select recipes that are high-carb in absolute terms and low-protein in absolute terms. This guarantees that the selected recipes are truly high in carbohydrate content and low in protein content, rather than just having a high ratio.\n",
    "\n",
    "To do this, we first need to convert protein and carbohydrate to calories. Each gram of carbohydrate or protein contains 4 calories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z7PlD7E5BJi"
   },
   "outputs": [],
   "source": [
    "carb_calories = recipe_ratings['carbohydrates'] * 4  # 4 calories per gram of carbs\n",
    "protein_calories = recipe_ratings['protein'] * 4  # 4 calories per gram of protein\n",
    "\n",
    "recipe_ratings['carb_prop'] = carb_calories / recipe_ratings['calories']\n",
    "recipe_ratings['protein_prop'] = protein_calories / recipe_ratings['calories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewgXzIAH5BJi"
   },
   "outputs": [],
   "source": [
    "carb_threshold = recipe_ratings['carb_prop'].quantile(0.75)\n",
    "protein_threshold = recipe_ratings['protein_prop'].quantile(0.25)\n",
    "\n",
    "recipe_ratings['high_carb_low_protein'] = (\n",
    "    (recipe_ratings['carb_prop'] >= carb_threshold) &\n",
    "    (recipe_ratings['protein_prop'] <= protein_threshold)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "031I0K9s5BJi"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN3SyBh-5BJi"
   },
   "source": [
    "We can see that protein_percent has a max value of 1.88 which is not possible, indicating an error. We will drop all such rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTXz0-Q65BJi"
   },
   "outputs": [],
   "source": [
    "recipe_ratings = recipe_ratings[(recipe_ratings['protein_prop'] <= 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWjXZT0x5BJi"
   },
   "source": [
    "Now we are ready to visualize our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3ZrAsa65BJi"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASnvJ17L5BJi"
   },
   "outputs": [],
   "source": [
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"browser\"  # IGNORE THIS FOR NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2fS2UPv5BJi"
   },
   "outputs": [],
   "source": [
    "#Distribution of Ratings\n",
    "px.histogram(recipe_ratings, x = 'avg_rating', nbins = 10, title = 'Distribution of Average Recipe Ratings').show()\n",
    "px.histogram(recipe_ratings, x = 'rating', nbins = 10, title = 'Distribution of Recipe Ratings').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB7zEcIM5BJi"
   },
   "source": [
    "The distribution of average ratings is **highly skewed to the left**.This suggests that most recipes receive **high ratings**, making it important to analyze which rating values appear most frequently and how they relate to other factors like the nutritional facts of the food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoaXfx7l5BJi"
   },
   "source": [
    "Most ratings left by people tend to be 5 stars.\n",
    "\n",
    "Now let's look at the distribution of Carbohydrate and Protein content of recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZF0PIws5BJi"
   },
   "outputs": [],
   "source": [
    "recipe_ratings['protein'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOlbDR7U5BJj"
   },
   "outputs": [],
   "source": [
    "recipe_ratings['carbohydrates'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xmbDofD5BJj"
   },
   "source": [
    "There appears to be very high values of carb and protein (over 3000) which seems unrealistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ItDNtCS5BJj"
   },
   "outputs": [],
   "source": [
    "fig_protein = px.box(recipe_ratings, x='protein', title='Boxplot of Protein Content')\n",
    "fig_protein.show()\n",
    "\n",
    "fig_carbs = px.box(recipe_ratings, x='carbohydrates', title='Boxplot of Carbohydrates Content')\n",
    "fig_carbs.show()\n",
    "\n",
    "fig_carbs = px.box(ratio_recipe_ratings, x='ratio_carb_protein', title='Boxplot of Carbohydrate to Protein Ratio Content')\n",
    "fig_carbs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEDh4nKP5BJj"
   },
   "outputs": [],
   "source": [
    "recipe_ratings[(recipe_ratings['carbohydrates'] > 200) | (recipe_ratings['carbohydrates'] > 200)].shape[0] / recipe_ratings.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyFfyiBO5BJj"
   },
   "source": [
    "Less than 0.3 % of the data has either protein or carbohydrate content over 200g, so we can leave the outliers as they are since they are not likely to affect our analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLH8dIFE5BJj"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFWVwR5P5BJj"
   },
   "source": [
    "We used a scatter plot for the the Ratio of Carbohydrates and Protein with ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDzqU8745BJj"
   },
   "outputs": [],
   "source": [
    "px.scatter(ratio_recipe_ratings, x=\"avg_rating\", y=\"ratio_carb_protein\",\n",
    "                 title=\"Ratio of Carbohydrates and Protein vs. Ratings\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv2PzMT45BJj"
   },
   "outputs": [],
   "source": [
    "px.scatter(recipe_ratings, x=\"avg_rating\", y=\"carbohydrates\",\n",
    "                 title=\"Carbohydrates vs. Ratings\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUJKkh8f5BJj"
   },
   "outputs": [],
   "source": [
    "px.scatter(recipe_ratings, x=\"avg_rating\", y=\"protein\",\n",
    "                 title=\"Protein vs. Ratings\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXnBYdA25BJj"
   },
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmLP6RHCC-Gz"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    },
    "id": "a2f7wO9d5BJj"
   },
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0jd6L6XARPq"
   },
   "outputs": [],
   "source": [
    "# Define missingness indicator for \"review\"\n",
    "merged_df[\"review_missing\"] = merged_df[\"review\"].isnull()\n",
    "\n",
    "# Number of repetitions\n",
    "n_repetitions = 500\n",
    "shuffled = merged_df.copy()\n",
    "\n",
    "tvds = []\n",
    "for _ in range(n_repetitions):\n",
    "\n",
    "    # Shuffling the missingness indicator for 'review'\n",
    "    shuffled[\"review_missing\"] = np.random.permutation(shuffled[\"review_missing\"])\n",
    "\n",
    "    # Computing and storing the TVD\n",
    "    pivoted = (\n",
    "        shuffled\n",
    "        .pivot_table(index=\"rating\", columns=\"review_missing\", aggfunc=\"size\")\n",
    "    )\n",
    "\n",
    "    pivoted = pivoted / pivoted.sum()\n",
    "\n",
    "    tvd = pivoted.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "    tvds.append(tvd)\n",
    "\n",
    "# Compute observed TVD\n",
    "observed_pivot = merged_df.pivot_table(index=\"rating\", columns=\"review_missing\", aggfunc=\"size\")\n",
    "observed_pivot = observed_pivot / observed_pivot.sum()\n",
    "observed_tvd = observed_pivot.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "\n",
    "# Create a histogram of TVD values\n",
    "fig = px.histogram(pd.DataFrame(tvds), x=0, nbins=50, histnorm=\"probability\",\n",
    "                   title=\"Empirical Distribution of the TVD\")\n",
    "\n",
    "# Add observed TVD as a red vertical line\n",
    "fig.add_vline(x=observed_tvd, line_color=\"red\", line_width=2, opacity=1)\n",
    "\n",
    "# Add annotation for observed TVD\n",
    "fig.add_annotation(text=f'<span style=\"color:red\">Observed TVD = {round(observed_tvd, 2)}</span>',\n",
    "                   x=2.5 * observed_tvd, showarrow=False, y=0.16)\n",
    "\n",
    "# Adjust axis range\n",
    "fig.update_layout(yaxis_range=[0, 0.2])\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8So0-z9mHECo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(np.array(tvds) >= observed_tvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqgHuVbOcohw"
   },
   "source": [
    "We **fail to reject the null**.\n",
    "This test does not provide evidence that the missingness in the 'review' column is dependent on 'rating' since 0.69 > 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBl1jSeDIaO1"
   },
   "outputs": [],
   "source": [
    "# Define missingness indicator for \"review\"\n",
    "merged_df[\"review_missing\"] = merged_df[\"review\"].isnull()\n",
    "\n",
    "# Number of repetitions\n",
    "n_repetitions = 500\n",
    "shuffled = merged_df.copy()\n",
    "\n",
    "tvds = []\n",
    "for _ in range(n_repetitions):\n",
    "\n",
    "    # Shuffling the missingness indicator for 'review'\n",
    "    shuffled[\"review_missing\"] = np.random.permutation(shuffled[\"review_missing\"])\n",
    "\n",
    "    # Computing and storing the TVD\n",
    "    pivoted = (\n",
    "        shuffled\n",
    "        .pivot_table(index=\"n_steps\", columns=\"review_missing\", aggfunc=\"size\")\n",
    "    )\n",
    "\n",
    "    pivoted = pivoted / pivoted.sum()\n",
    "\n",
    "    tvd = pivoted.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "    tvds.append(tvd)\n",
    "\n",
    "# Compute observed TVD\n",
    "observed_pivot = merged_df.pivot_table(index=\"n_steps\", columns=\"review_missing\", aggfunc=\"size\")\n",
    "observed_pivot = observed_pivot / observed_pivot.sum()\n",
    "observed_tvd = observed_pivot.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "\n",
    "# Compute p-value\n",
    "p_value = np.mean(np.array(tvds) >= observed_tvd)\n",
    "\n",
    "# Create a histogram of TVD values\n",
    "fig = px.histogram(pd.DataFrame(tvds), x=0, nbins=50, histnorm=\"probability\",\n",
    "                   title=\"Empirical Distribution of the TVD for n_steps\")\n",
    "\n",
    "# Add observed TVD as a red vertical line\n",
    "fig.add_vline(x=observed_tvd, line_color=\"red\", line_width=2, opacity=1)\n",
    "\n",
    "# Add annotation for observed TVD\n",
    "fig.add_annotation(text=f'<span style=\"color:red\">Observed TVD = {round(observed_tvd, 2)}</span>',\n",
    "                   x=2.5 * observed_tvd, showarrow=False, y=0.16)\n",
    "\n",
    "# Adjust axis range\n",
    "fig.update_layout(yaxis_range=[0, 0.2])\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSxTj0KWLTKG"
   },
   "outputs": [],
   "source": [
    "np.mean(np.array(tvds) >= observed_tvd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **reject the null**.\n",
    "This test provide evidence that the missingness in the 'review' column is dependent on 'n_steps' since 0.118 < 0.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5jz5OK_5BJj"
   },
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    },
    "id": "Af50wpNv5BJj"
   },
   "source": [
    "Our goal is to see if carbohydrate and protein content affect ratings of recipes. We define high-carb, low-protein recipes as those that fall into both:\n",
    "\n",
    "- The top 25th percentile for the proportion of calories from carbohydrates\n",
    "- The bottom 25th percentile for the proportion of calories from protein\n",
    "\n",
    "**Null Hypothesis (H₀):** Recipes with high carb % and low protein % receive the same ratings as other recipes.  \n",
    "\n",
    "**Alternative Hypothesis (Hₐ):** Recipes with high carb % and low protein % receive significantly different ratings.\n",
    "\n",
    "**Test statistic:** Mean difference in ratings between the high-carb, low-protein group and others.\n",
    "\n",
    "**Significance level:** 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh3EkZhz5BJk"
   },
   "outputs": [],
   "source": [
    "observed_diff = recipe_ratings.groupby(\"high_carb_low_protein\")[\"avg_rating\"].mean().diff().iloc[-1]\n",
    "\n",
    "def permute_ratings(df):\n",
    "    shuffled = df[\"avg_rating\"].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    df[\"shuffled_rating\"] = shuffled\n",
    "    return df.groupby(\"high_carb_low_protein\")[\"shuffled_rating\"].mean().diff().iloc[-1]\n",
    "\n",
    "perm_diffs = [permute_ratings(recipe_ratings) for _ in range(1000)]\n",
    "\n",
    "p_value = np.mean(np.array(perm_diffs) >= observed_diff)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu58Fbil5BJk"
   },
   "source": [
    "Since the p-value is less than the significance level 0.05, we reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjd_PyAb5BJk"
   },
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the ratings of recipes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    },
    "id": "FA5ZPyE-5BJk"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cOT86D45BJk"
   },
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    },
    "id": "J9qQ5IVh5BJk"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropna()\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['log_minutes'] = np.log1p(merged_df['minutes'])\n",
    "scaler = MinMaxScaler()\n",
    "merged_df['scaled_n_steps'] = scaler.fit_transform(merged_df[['n_steps']])\n",
    "\n",
    "features = ['log_minutes', 'scaled_n_steps']\n",
    "target = 'rating'\n",
    "\n",
    "X = merged_df[features]\n",
    "y = merged_df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_, model.coef_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model coefficients\n",
    "intercept = model.intercept_\n",
    "coef_log_minutes, coef_scaled_steps = model.coef_  # Coefficients for transformed features\n",
    "\n",
    "# Create structured grid using np.mgrid[]\n",
    "X_range = np.linspace(X_test['log_minutes'].min(), X_test['log_minutes'].max(), 50)\n",
    "Y_range = np.linspace(X_test['scaled_n_steps'].min(), X_test['scaled_n_steps'].max(), 50)\n",
    "XX, YY = np.mgrid[X_range.min():X_range.max():50j, Y_range.min():Y_range.max():50j]\n",
    "\n",
    "# Compute regression plane using equation: Z = intercept + coef_1 * X + coef_2 * Y\n",
    "Z = intercept + coef_log_minutes * XX + coef_scaled_steps * YY\n",
    "\n",
    "# Create 3D plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add regression plane\n",
    "fig.add_trace(go.Surface(\n",
    "    x=XX, \n",
    "    y=YY, \n",
    "    z=Z, \n",
    "    colorscale=\"Oranges\", \n",
    "    opacity=0.7,\n",
    "    name=\"Regression Plane\"\n",
    "))\n",
    "\n",
    "# Add actual ratings scatter plot\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_test['log_minutes'], \n",
    "    y=X_test['scaled_n_steps'], \n",
    "    z=y_test, \n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='#656DF1', opacity=0.7),\n",
    "    name=\"Actual Ratings\"\n",
    "))\n",
    "\n",
    "# Update layout to match reference\n",
    "fig.update_layout(\n",
    "    title=\"Regression Plane: Log(Minutes) vs. Scaled Steps\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Log(Minutes)\",\n",
    "        yaxis_title=\"Scaled Steps\",\n",
    "        zaxis_title=\"Predicted Rating\"\n",
    "    ),\n",
    "    width=500, height=500\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHifXL6O5BJk"
   },
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    },
    "id": "yhDLpnGk5BJk"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36EDTNsf5BJk"
   },
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    },
    "id": "gp6q9WeE5BJk"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
