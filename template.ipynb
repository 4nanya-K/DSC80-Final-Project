{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNht40dW_P8I",
    "outputId": "54a815cb-d15e-4c02-a122-0be84b1f3196"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCt1QY2E5BJW"
   },
   "source": [
    "# Macronutrients & Ratings: \n",
    "# Impact of High-Carbs and Low-Protein on Recipe Ratings\n",
    "\n",
    "**Name(s)**: Ananya Krishnan, John Wesley Pabalate\n",
    "\n",
    "**Website Link**: https://j0hnwesl3yyy.github.io/recipe-rating-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    },
    "id": "U_OBhvSp5BJX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "#from dsc80_utils import * # Feel free to uncomment and use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqCArYDF5BJY"
   },
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Do high-carb, low-protein recipes receive significantly different ratings compared to other recipes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pxNfkQe5BJY"
   },
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "6SYdl1zN5BJY",
    "outputId": "b113df0d-d05d-471f-ea47-5ac64c8d3884"
   },
   "outputs": [],
   "source": [
    "#JohnWesley's Directory\n",
    "#recipes = pd.read_csv('/Users/johnwesleypabalate/Desktop/dsc80-2025-wi/projects/project04/data/RAW_recipes.csv')\n",
    "\n",
    "#Ananya's Directory\n",
    "recipes = pd.read_csv('/Users/ananyakrishnan/Downloads/DSC80/projects/project04/data/RAW_recipes.csv')\n",
    "recipes.head()\n",
    "\n",
    "#Colab Directory\n",
    "# recipes = pd.read_csv('/content/RAW_recipes.csv')\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fzm4nVcb5BJY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#JohnWesley's Directory\n",
    "#reviews = pd.read_csv('/Users/johnwesleypabalate/Desktop/dsc80-2025-wi/projects/project04/data/RAW_interactions.csv')\n",
    "\n",
    "#Ananya's Directory\n",
    "reviews = pd.read_csv('../DSC80/projects/project04/data/RAW_interactions.csv')\n",
    "reviews.head()\n",
    "\n",
    "#Colab Directory\n",
    "# reviews = pd.read_csv('/content/interactions.csv')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj3R7Gms5BJY"
   },
   "source": [
    "Let us now merge recipes and ratings into one comprehensive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q813voGP5BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings = recipes.merge(reviews, left_on = 'id', right_on = 'recipe_id', how=\"left\")\n",
    "recipe_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3AtXzes5BJZ"
   },
   "source": [
    "Let us replace all 0s in the ratings column with NaN values. The 0 represents no rating given, but it will influence any calculations we perform with the ratings. We also calculate the average ratings for each recipe and store it in `avg_recipe_rating`. We will then add it as a column to recipe_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyIrGLYd5BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.loc[recipe_ratings['rating'] == 0, 'rating'] = np.nan\n",
    "avg_recipe_rating = recipe_ratings.groupby('recipe_id')['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyTvTB_y5BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings = recipe_ratings.merge(avg_recipe_rating.reset_index().rename(columns={'rating': 'avg_rating'}), on = 'recipe_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Made a copy of recipe_ratings for the purpose of using it for the Missingness and Baseline: \n",
    "# merged_df = recipe_ratings.copy()\n",
    "# merged_df = merged_df.drop(columns=['Unnamed: 0'])\n",
    "# merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmih-YBt5BJZ"
   },
   "source": [
    "There are many columns not relevant to our question, so we will clean only the columns related to recipe id, nutrition information and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "herKkG_75BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings = recipe_ratings.drop(columns = ['recipe_id']).rename(columns = {'id': 'recipe_id'}).drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmhddbvL5BJZ"
   },
   "source": [
    "Let us now look at the columns we have and clean them up one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_uUcZ075BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atH8uKyK5BJZ"
   },
   "source": [
    "We observe that `nutrition` actually contains strings formatted to look like lists, so let us convert it to real lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    },
    "id": "O7Hpp81j5BJZ"
   },
   "outputs": [],
   "source": [
    "recipe_ratings['nutrition'] = recipe_ratings['nutrition'].str.strip('[').str.strip(']').str.replace(\"'\", \"\").str.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ire9fL1e5BJZ"
   },
   "source": [
    "The `nutrition` column now contains lists of values. Let us separate each value into its respective category - `'calories'`, `'total_fat'`, `'sugar'`, `'sodium'`, `'protein'`, `'saturated_fat'` and `'carbohydrates'`. We can then drop the `nutrition` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDMwYimz5BJa"
   },
   "outputs": [],
   "source": [
    "categories = ['calories', 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbohydrates']\n",
    "recipe_ratings = recipe_ratings.assign(\n",
    "    **{category: pd.to_numeric(recipe_ratings['nutrition'].str[i], errors='coerce') for i, category in enumerate(categories)}\n",
    ")\n",
    "recipe_ratings = recipe_ratings.drop(columns = ['nutrition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJtJCY025BJa"
   },
   "source": [
    "Let us now look at our cleaned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9pRY0ba5BJa"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5n-5JB45BJa"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XPjkiX_5BJa"
   },
   "source": [
    "The nutritional values seem to have abnormally high max values despite a reasonable mean. Let us look at the rows with high protein or high carbohydrate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwVNd8_X5BJa"
   },
   "outputs": [],
   "source": [
    "recipe_ratings[(recipe_ratings['protein'] > 200) | (recipe_ratings['carbohydrates'] > 200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K9OoPJK5BJa"
   },
   "source": [
    "These rows have proportionally high calories, meaning this is unlikely to be an error and could just be because of large portion sizes. We can leave it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_eH58cA5BJb"
   },
   "source": [
    "We want to account for the percentge of calories that the protein and carbohydrate contribute to. We will define high carbohydrate - low protein recipes using arbitrary cutoffs, considering those in the top 25th percentile of carb percentage and bottom 25th percentile of protein percentage. Using percentiles ensures that we select recipes that are high-carb in absolute terms and low-protein in absolute terms. This guarantees that the selected recipes are truly high in carbohydrate content and low in protein content, rather than just having a high ratio.\n",
    "\n",
    "To do this, we first need to convert protein and carbohydrate to calories. Each gram of carbohydrate or protein contains 4 calories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z7PlD7E5BJi"
   },
   "outputs": [],
   "source": [
    "carb_calories = recipe_ratings['carbohydrates'] * 4  # 4 calories per gram of carbs\n",
    "protein_calories = recipe_ratings['protein'] * 4  # 4 calories per gram of protein\n",
    "\n",
    "recipe_ratings['carb_prop'] = carb_calories / recipe_ratings['calories']\n",
    "recipe_ratings['protein_prop'] = protein_calories / recipe_ratings['calories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewgXzIAH5BJi"
   },
   "outputs": [],
   "source": [
    "carb_threshold = recipe_ratings['carb_prop'].quantile(0.75)\n",
    "protein_threshold = recipe_ratings['protein_prop'].quantile(0.25)\n",
    "\n",
    "recipe_ratings['high_carb_low_protein'] = (\n",
    "    (recipe_ratings['carb_prop'] >= carb_threshold) &\n",
    "    (recipe_ratings['protein_prop'] <= protein_threshold)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "031I0K9s5BJi"
   },
   "outputs": [],
   "source": [
    "recipe_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN3SyBh-5BJi"
   },
   "source": [
    "We can see that protein_percent has a max value of 1.88 which is not possible, indicating an error. We will drop all such rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTXz0-Q65BJi"
   },
   "outputs": [],
   "source": [
    "recipe_ratings = recipe_ratings[(recipe_ratings['protein_prop'] <= 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWjXZT0x5BJi"
   },
   "source": [
    "Now we are ready to visualize our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dicnjy3K5BJa"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create and save the histogram with the distribution of average rating\n",
    "fig1_uni = px.histogram(recipe_ratings, x='avg_rating', nbins=10, title='Distribution of Average Recipe Ratings')\n",
    "fig1_uni.write_html(\"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/dist_avg_rating_histogram.html\", include_plotlyjs=\"cdn\")\n",
    "fig1_uni.show()\n",
    "\n",
    "# Create and save the second histogram with the distribution of the protein proportions\n",
    "fig2_uni = px.histogram(recipe_ratings, x='protein_prop', nbins=10, title='Distribution of Protein Proportion in Recipes')\n",
    "fig2_uni.write_html(\"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/dist_protein_prop_histogram.html\", include_plotlyjs=\"cdn\")\n",
    "fig2_uni.show()\n",
    "\n",
    "#Create and save the third histogram with the distribution of the carbohydrates proportions\n",
    "fig3_uni = px.histogram(recipe_ratings, x='carb_prop', nbins=10, title='Distribution of Carbohydrates Proportion in Recipes')\n",
    "fig3_uni.write_html(\"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/dist_carb_prop_histogram.html\", include_plotlyjs=\"cdn\")\n",
    "fig3_uni.show()\n",
    "\n",
    "fig4_uni = px.histogram(recipe_ratings, x='high_carb_low_protein', nbins=10, title='Distribution of High-Carbs & Low-Protein in Recipes')\n",
    "fig4_uni.write_html(\"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/dist_highcarb_lowprotein_histogram.html\", include_plotlyjs=\"cdn\")\n",
    "fig4_uni.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_protein = px.box(recipe_ratings, x='protein', title='Boxplot of Protein Content')\n",
    "fig_protein.show()\n",
    "\n",
    "fig_carbs = px.box(recipe_ratings, x='carbohydrates', title='Boxplot of Carbohydrates Content')\n",
    "fig_carbs.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nluSRsw_5BJa"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the a density heatmap between the average rating and the proportion of carbohydrates\n",
    "fig_heat1 = px.density_heatmap(recipe_ratings, x=\"avg_rating\", y=\"carb_prop\",\n",
    "                         title=\"Heatmap of Carbohydrate Proportion vs. Average Ratings\",\n",
    "                         nbinsx=10, nbinsy=30, color_continuous_scale=\"Blues\") \n",
    "#Turns the plot into html format\n",
    "fig_heat1.write_html(\"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/carb_corp_heat.html\", include_plotlyjs=\"cdn\")\n",
    "\n",
    "#Outputs the visualization\n",
    "fig_heat1.show()\n",
    "\n",
    "#Creates the a density heatmap between the average rating and the proportion of protein\n",
    "fig_heat2 = px.density_heatmap(recipe_ratings, x=\"avg_rating\", y=\"protein_prop\",\n",
    "                         title=\"Heatmap of Protein Proportion vs. Average Ratings\",\n",
    "                         nbinsx=10, nbinsy=30, color_continuous_scale=\"Blues\")\n",
    "\n",
    "#Turns the plot into html format\n",
    "fig_heat2.write_html(\"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/protein_prop_heat.html\", include_plotlyjs=\"cdn\")\n",
    "\n",
    "#Outputs the visualization\n",
    "fig_heat2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semething interesting to see would be if carbohydrate and protein content vary by how long it takes to make a recipe. Do shorter recipes tend to be carb-heavy while longer recipes are more balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_ratings['cooking_time_category'] = pd.cut(recipe_ratings['minutes'], \n",
    "                                                 bins=[0, 30, 60, 120, recipe_ratings['minutes'].max()], \n",
    "                                                 labels=['<30 min', '30-60 min', '60-120 min', '120+ min'],\n",
    "                                                 include_lowest=True)\n",
    "\n",
    "agg_cooking_time = recipe_ratings.groupby('cooking_time_category').agg(\n",
    "    avg_rating=('avg_rating', 'mean'),\n",
    "    avg_carb_prop=('carb_prop', 'mean'),\n",
    "    avg_protein_prop=('protein_prop', 'mean'),\n",
    "    count=('recipe_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "agg_cooking_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(agg_cooking_time, x='cooking_time_category', y=['avg_carb_prop', 'avg_protein_prop'],\n",
    "             title='Average Macronutrient Proportions by Cooking Time Category',\n",
    "             labels={'value': 'Proportion', 'cooking_time_category': 'Cooking Time Category'},\n",
    "             barmode='group')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, it looks like the carbohydrate content stays around the same through dfferent cooking time categories, but protein content seems to increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCYmEWJ45BJa"
   },
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    },
    "id": "0xjA6ww05BJb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recipe_ratings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure missingness indicators exist\n",
    "recipe_ratings[\"rating_missing\"] = recipe_ratings[\"rating\"].isnull()\n",
    "recipe_ratings[\"review_missing\"] = recipe_ratings[\"review\"].isnull()\n",
    "recipe_ratings[\"description_missing\"] = recipe_ratings[\"description\"].isnull()\n",
    "\n",
    "#Set the number of permutations\n",
    "n_repetitions = 500\n",
    "\n",
    "#Initialize empty lists to store TVD values\n",
    "tvds_review = []\n",
    "tvds_desc = []\n",
    "\n",
    "#Compute observed TVD for review_missing\n",
    "observed_pivot_review = recipe_ratings.pivot_table(\n",
    "    index=\"review_missing\", \n",
    "    columns=\"rating_missing\", \n",
    "    aggfunc=\"size\", \n",
    "    fill_value=0\n",
    ")\n",
    "observed_pivot_review = observed_pivot_review / observed_pivot_review.sum()\n",
    "observed_tvd_review = observed_pivot_review.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "\n",
    "#Compute observed TVD for description_missing\n",
    "observed_pivot_desc = recipe_ratings.pivot_table(\n",
    "    index=\"description_missing\", \n",
    "    columns=\"rating_missing\", \n",
    "    aggfunc=\"size\", \n",
    "    fill_value=0\n",
    ")\n",
    "observed_pivot_desc = observed_pivot_desc / observed_pivot_desc.sum()\n",
    "observed_tvd_desc = observed_pivot_desc.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "\n",
    "#Permutation test for review_missing\n",
    "shuffled = recipe_ratings.copy()\n",
    "for _ in range(n_repetitions):\n",
    "    shuffled[\"rating_missing\"] = np.random.permutation(shuffled[\"rating_missing\"])\n",
    "    \n",
    "    pivoted = shuffled.pivot_table(\n",
    "        index=\"review_missing\", \n",
    "        columns=\"rating_missing\", \n",
    "        aggfunc=\"size\", \n",
    "        fill_value=0\n",
    "    )\n",
    "    pivoted = pivoted / pivoted.sum()\n",
    "    \n",
    "    tvd = pivoted.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "    tvds_review.append(tvd)\n",
    "\n",
    "#Permutation test for description_missing\n",
    "shuffled = recipe_ratings.copy()\n",
    "for _ in range(n_repetitions):\n",
    "    shuffled[\"rating_missing\"] = np.random.permutation(shuffled[\"rating_missing\"])\n",
    "    \n",
    "    pivoted = shuffled.pivot_table(\n",
    "        index=\"description_missing\", \n",
    "        columns=\"rating_missing\", \n",
    "        aggfunc=\"size\", \n",
    "        fill_value=0\n",
    "    )\n",
    "    pivoted = pivoted / pivoted.sum()\n",
    "    \n",
    "    tvd = pivoted.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "    tvds_desc.append(tvd)\n",
    "\n",
    "#Compute p-values\n",
    "p_value_review = np.mean(np.array(tvds_review) >= observed_tvd_review)\n",
    "p_value_desc = np.mean(np.array(tvds_desc) >= observed_tvd_desc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure that permutation test has already run\n",
    "if \"tvds_review\" not in locals() or \"tvds_desc\" not in locals():\n",
    "    raise NameError(\"Run the permutation test first to define tvds_review and tvds_desc.\")\n",
    "\n",
    "#Define the number of bins\n",
    "num_bins = 100  \n",
    "\n",
    "#Create subplots for Review and Description Missingness\n",
    "fig1 = go.Figure()\n",
    "fig2 = go.Figure()\n",
    "\n",
    "#Plot for Review Missingness --- ###\n",
    "fig1.add_trace(go.Histogram(\n",
    "    x=tvds_review,\n",
    "    nbinsx=num_bins,\n",
    "    histnorm='probability density',\n",
    "    opacity=0.7,\n",
    "    marker_color=\"blue\",\n",
    "    name=\"Permutation TVD (Review Missing)\"\n",
    "))\n",
    "\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=[observed_tvd_review, observed_tvd_review],  \n",
    "    y=[0, max(np.histogram(tvds_review, bins=num_bins, density=True)[0])], \n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"red\", width=2),\n",
    "    name=f\"Observed TVD: {observed_tvd_review:.4f}\"\n",
    "))\n",
    "\n",
    "fig1.update_layout(\n",
    "    title=f\"Empirical Distribution of TVD for Review Missingness\\np-value = {p_value_review:.3f}\",\n",
    "    xaxis_title=\"TVD\",\n",
    "    yaxis_title=\"Probability\",\n",
    "    barmode=\"overlay\",\n",
    "    bargap=0  \n",
    ")\n",
    "\n",
    "#Plot for Description Missingness \n",
    "fig2.add_trace(go.Histogram(\n",
    "    x=tvds_desc,\n",
    "    nbinsx=num_bins,\n",
    "    histnorm='probability density',\n",
    "    opacity=0.7,\n",
    "    marker_color=\"brown\",\n",
    "    name=\"Permutation TVD (Description Missing)\"\n",
    "))\n",
    "\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=[observed_tvd_desc, observed_tvd_desc],  \n",
    "    y=[0, max(np.histogram(tvds_desc, bins=num_bins, density=True)[0])], \n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"red\", width=2),\n",
    "    name=f\"Observed TVD: {observed_tvd_desc:.4f}\"\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=f\"Empirical Distribution of TVD for Description Missingness\\np-value = {p_value_desc:.3f}\",\n",
    "    xaxis_title=\"TVD\",\n",
    "    yaxis_title=\"Probability\",\n",
    "    barmode=\"overlay\",\n",
    "    bargap=0  \n",
    ")\n",
    "\n",
    "#Save each figure as an interactive HTML file\n",
    "pio.write_html(fig1, file=\"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/review_missingness.html\", include_plotlyjs=\"cdn\")\n",
    "pio.write_html(fig2, file=\"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/description_missingness.html\", include_plotlyjs=\"cdn\")\n",
    "\n",
    "#Display the plots\n",
    "fig1.show()\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter dataset based on whether 'description' is missing or not\n",
    "df_missing_desc = recipe_ratings[recipe_ratings[\"description_missing\"] == True][\"rating\"].dropna()\n",
    "df_notmissing_desc = recipe_ratings[recipe_ratings[\"description_missing\"] == False][\"rating\"].dropna()\n",
    "\n",
    "# Create KDE plot in Plotly\n",
    "fig = ff.create_distplot(\n",
    "    [df_missing_desc, df_notmissing_desc],  # Data for KDE\n",
    "    group_labels=[\"Description Missing\", \"Description Not Missing\"],  # Labels\n",
    "    show_hist=False,  # Hide histogram, show only KDE\n",
    "    show_rug=True  # Show small tick marks for data points\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"Kernel Density Estimate of Rating by Description Missingness\",\n",
    "    xaxis_title=\"Rating\",\n",
    "    yaxis_title=\"Density\"\n",
    ")\n",
    "\n",
    "#Save as interactive HTML\n",
    "html_path = \"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/desc_rate_kde.html\"\n",
    "pio.write_html(fig, file=html_path, include_plotlyjs=\"cdn\")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter dataset based on whether 'review' is missing or not\n",
    "df_missing_review = recipe_ratings[recipe_ratings[\"review_missing\"] == True][\"rating\"].dropna()\n",
    "df_notmissing_review = recipe_ratings[recipe_ratings[\"review_missing\"] == False][\"rating\"].dropna()\n",
    "\n",
    "#Create KDE plot in Plotly\n",
    "fig = ff.create_distplot(\n",
    "    [df_missing_review, df_notmissing_review],  # Data for KDE\n",
    "    group_labels=[\"Review Missing\", \"Review Not Missing\"],  # Labels\n",
    "    show_hist=False,  # Hide histogram, show only KDE\n",
    "    show_rug=True  # Show small tick marks for data points\n",
    ")\n",
    "\n",
    "#Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"Kernel Density Estimate of Rating by Review Missingness\",\n",
    "    xaxis_title=\"Rating\",\n",
    "    yaxis_title=\"Density\"\n",
    ")\n",
    "\n",
    "#Save as interactive HTML\n",
    "html_path = \"/Users/johnwesleypabalate/Desktop/recipe-rating-analysis/assets/review_rate_kde.html\"\n",
    "pio.write_html(fig, file=html_path, include_plotlyjs=\"cdn\")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5jz5OK_5BJj"
   },
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    },
    "id": "Af50wpNv5BJj"
   },
   "source": [
    "Our goal is to see if carbohydrate and protein content affect ratings of recipes. We define high-carb, low-protein recipes as those that fall into both:\n",
    "\n",
    "- The top 25th percentile for the proportion of calories from carbohydrates\n",
    "- The bottom 25th percentile for the proportion of calories from protein\n",
    "\n",
    "**Null Hypothesis (H₀):** Recipes with high carb % and low protein % receive the same ratings as other recipes.  \n",
    "\n",
    "**Alternative Hypothesis (Hₐ):** Recipes with high carb % and low protein % receive significantly different ratings.\n",
    "\n",
    "**Test statistic:** Mean difference in ratings between the high-carb, low-protein group and others.\n",
    "\n",
    "**Significance level:** 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh3EkZhz5BJk"
   },
   "outputs": [],
   "source": [
    "observed_diff = recipe_ratings.groupby(\"high_carb_low_protein\")[\"avg_rating\"].mean().diff().iloc[-1]\n",
    "\n",
    "def permute_ratings(df):\n",
    "    shuffled = df[\"avg_rating\"].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    df[\"shuffled_rating\"] = shuffled\n",
    "    return df.groupby(\"high_carb_low_protein\")[\"shuffled_rating\"].mean().diff().iloc[-1]\n",
    "\n",
    "perm_diffs = [permute_ratings(recipe_ratings) for _ in range(1000)]\n",
    "\n",
    "p_value = np.mean(np.array(perm_diffs) >= observed_diff)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu58Fbil5BJk"
   },
   "source": [
    "Since the p-value is less than the significance level 0.05, we reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjd_PyAb5BJk"
   },
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the ratings of recipes using different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cOT86D45BJk"
   },
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use all data we have engineered along with the original provided data. Let us first create some new features of interest:\n",
    "\n",
    "`'steps_per_minute'`: We will create this column to put into perspective the number of steps required for the duration of a recipe. This tells us whether it requires higher effort (having to perform several steps quickly). Effort may be related to how a recipe is rated. We will use this for our final analysis.\n",
    "\n",
    "Let us also convert 'high_carb_low_protein' columnn into a binary column to make our analyses easier. We will then drop all null values so that they dont affect the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recipe_ratings['steps_per_minute'] = recipe_ratings['n_steps'] / (recipe_ratings['minutes'] + 1e-6)\n",
    "recipe_ratings['high_carb_low_protein'] = recipe_ratings['high_carb_low_protein'].astype(int)\n",
    "recipe_ratings = recipe_ratings.dropna()\n",
    "\n",
    "recipe_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recipe_ratings['steps_per_minute'] = recipe_ratings['n_steps'] / (recipe_ratings['minutes'] + 1e-6)\n",
    "recipe_ratings['high_carb_low_protein'] = recipe_ratings['high_carb_low_protein'].astype(int)\n",
    "recipe_ratings = recipe_ratings.dropna()\n",
    "\n",
    "recipe_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our baseline model, we will use 'minutes' (quantitative, numerical), 'n_ingredients'(quantitative, numerical) and 'high_carb_low_protein'(quantitative, nominal) in a Random Forest Regressor model to predict 'avg_rating'. \n",
    "\n",
    "We want to use these features because we believe the recipes that take very long may get lower ratings while higher number of ingredients may make the recipe more complex and taste better, receiving higher ratings. From our previous analyses we saw that high carb, low protein recipes tend to get higher ratings, so we beleive these would be a good predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize the quantitative features using StandardScaler to ensure they are on a comparable scale. The categorical feature is left as-is since it is already binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = ['minutes', 'n_ingredients', 'high_carb_low_protein']\n",
    "final_features = ['high_carb_low_protein', 'steps_per_minute', 'carb_prop', 'protein_prop']\n",
    "\n",
    "y = recipe_ratings['avg_rating'] \n",
    "X = recipe_ratings.drop(columns=['avg_rating', 'rating']) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_baseline = X_train[baseline_features]\n",
    "X_test_baseline = X_test[baseline_features]\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]\n",
    "\n",
    "preprocessor_baseline = ColumnTransformer([\n",
    "    ('num', StandardScaler(), ['minutes', 'n_ingredients']) \n",
    "], remainder='passthrough')\n",
    "\n",
    "preprocessor_final = ColumnTransformer([\n",
    "    ('num', StandardScaler(), ['steps_per_minute', 'carb_prop', 'protein_prop']),  \n",
    "], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_baseline),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_jobs=-1))  \n",
    "])\n",
    "\n",
    "baseline_pipeline.fit(X_train_baseline, y_train)\n",
    "\n",
    "y_pred_baseline = baseline_pipeline.predict(X_test_baseline)\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"Baseline MAE: {mae_baseline:.4f}\")\n",
    "print(f\"Baseline R²: {r2_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHifXL6O5BJk"
   },
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final model, we will use more nutrition related features, since they might be more likely to affect quality of food and hence the ratings:\n",
    "'high_carb_low_protein'(quantitative, nominal), 'steps_per_minute'(quantitative, numerical), 'carb_prop'(quantitative, numerical), 'protein_prop'(quantitative, numerical).\n",
    "We standardize the numerical features using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_final),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [5, 10, 15],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "random_search_final = RandomizedSearchCV(\n",
    "    final_pipeline, param_distributions=param_dist, n_iter=10,\n",
    "    cv=3, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\n Running Final Model Hyperparameter Search...\")\n",
    "random_search_final.fit(X_train_final, y_train)\n",
    "\n",
    "y_pred_final = random_search_final.best_estimator_.predict(X_test_final)\n",
    "mae_final = mean_absolute_error(y_test, y_pred_final)\n",
    "r2_final = r2_score(y_test, y_pred_final)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", random_search_final.best_params_)\n",
    "print(f\"Final Model MAE: {mae_final:.4f}\")\n",
    "print(f\"Final Model R²: {r2_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that MAE reduced and R² increased. This means that our final model improved. Below, we can see what the most important features were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search_final.best_estimator_\n",
    "feature_importance = best_model.named_steps['model'].feature_importances_\n",
    "final_feature_names = best_model[:-1].get_feature_names_out()\n",
    "\n",
    "print(\"Extracted Features:\", final_feature_names)\n",
    "print(\"Feature Importance Length:\", len(feature_importance))\n",
    "\n",
    "clean_feature_names = [name.replace(\"num__\", \"\").replace(\"remainder__\", \"\") for name in final_feature_names]\n",
    "\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': clean_feature_names,  \n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "fig = px.bar(\n",
    "    importance_df, \n",
    "    x='Importance', \n",
    "    y='Feature', \n",
    "    text=importance_df['Importance'].round(4), \n",
    "    title=\"Feature Importance in Final Model\"\n",
    ")\n",
    "fig.update_traces(marker_color='royalblue', textposition='outside')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36EDTNsf5BJk"
   },
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    },
    "id": "Af50wpNv5BJj"
   },
   "source": [
    "The goal of this analysis is to determine whether our model exhibits bias in predicting recipe ratings based on the carb proportion (carb_prop) in a recipe. We split recipes into two groups:\n",
    "\n",
    "High-carb recipes: Those with a carb_prop greater than the mean.\n",
    "Low-carb recipes: Those with a carb_prop less than or equal to the mean.\n",
    "We evaluate whether the model's performance (Mean Absolute Error, MAE) differs significantly between these two groups.\n",
    "\n",
    "\n",
    "**Null Hypothesis:** The model is fair. Its MAE for high-carb and low-carb recipes is roughly the same, and any differences are due to random chance.\n",
    "\n",
    "**Alternative Hypothesis:** The model is unfair. It performs significantly better (lower MAE) for low-carb recipes compared to high-carb recipes.\n",
    "\n",
    "**Significance level:** 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = np.mean([diff <= observed_diff for diff in diff_permutations])  # Test if low carb has lower MAE\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=diff_permutations,\n",
    "    nbinsx=30,\n",
    "    marker=dict(color='blue', opacity=0.7),\n",
    "    name='Permutation Differences'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[observed_diff, observed_diff],\n",
    "    y=[0, max(np.histogram(diff_permutations, bins=30)[0])],\n",
    "    mode=\"lines\",\n",
    "    line=dict(color='red', dash='dash'),\n",
    "    name=\"Observed Difference\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Empirical Distribution of the Difference in MAE (Low Carb - High Carb)\",\n",
    "    xaxis_title=\"Difference in MAE\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    legend_title=\"Legend\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The model performs significantly better for recipes with low carb_prop.\")\n",
    "else:\n",
    "    print(\"No significant evidence that the model performs better for low carb recipes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
